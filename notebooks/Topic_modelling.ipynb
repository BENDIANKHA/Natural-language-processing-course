{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling and Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic modeling is a common NLP task, which attempts to find the topics within a text document. Topic modeling is an unsupervised approach, and topic modeling only gives you an idea of which words frequently occur. It is then up to you to deduce the topic having seen the frequently and co-occurring words.\n",
    "\n",
    "One of the primary applications of natural language processing is to automatically extract what topics people are discussing from large volumes of text. Some examples of large text could be feeds from social media, customer reviews of hotels, movies, etc, user feedbacks, news stories, e-mails of customer complaints etc.\n",
    "\n",
    "Knowing what people are talking about and understanding their problems and opinions is highly valuable to businesses, administrators, political campaigns. And it’s really hard to manually read through such large volumes and compile the topics.\n",
    "\n",
    "Thus is required an automated algorithm that can read through the text documents and automatically output the topics discussed.\n",
    "\n",
    "In this tutorial, we will take a real example of the ’20 Newsgroups’ dataset and use LDA to extract the naturally discussed topics.\n",
    "\n",
    "I will be using the Latent Dirichlet Allocation (LDA) from Gensim package along with the Mallet’s implementation (via Gensim). Mallet has an efficient implementation of the LDA. It is known to run faster and gives better topics segregation.\n",
    "\n",
    "We will also extract the volume and percentage contribution of each topic to get an idea of how important a topic is. \n",
    "\n",
    "\n",
    "LDA’s approach to topic modeling is it considers each document as a collection of topics in a certain proportion. And each topic as a collection of keywords, again, in a certain proportion.\n",
    "\n",
    "Once you provide the algorithm with the number of topics, all it does it to rearrange the topics distribution within the documents and keywords distribution within the topics to obtain a good composition of topic-keywords distribution.\n",
    "\n",
    "When I say topic, what is it actually and how it is represented?\n",
    "\n",
    "A topic is nothing but a collection of dominant keywords that are typical representatives. Just by looking at the keywords, you can identify what the topic is all about.\n",
    "\n",
    "The following are key factors to obtaining good segregation topics:\n",
    "\n",
    "    The quality of text processing.\n",
    "    The variety of topics the text talks about.\n",
    "    The choice of topic modeling algorithm.\n",
    "    The number of topics fed to the algorithm.\n",
    "    The algorithms tuning parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- For this exercise we use the news headlines published over a period of eighteen years from Australian sources ABC from [kaggle](https://www.kaggle.com/therohk/million-headlines/data) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "# import pyLDAvis\n",
    "# import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/abcnews-date-text.csv', error_bad_lines=False);\n",
    "data_text = data[['headline_text']]\n",
    "data_text['index'] = data_text.index\n",
    "documents = data_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20030219</td>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20030219</td>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20030219</td>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20030219</td>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20030219</td>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   publish_date                                      headline_text\n",
       "0      20030219  aba decides against community broadcasting lic...\n",
       "1      20030219     act fire witnesses must be aware of defamation\n",
       "2      20030219     a g calls for infrastructure protection summit\n",
       "3      20030219           air nz staff in aust strike for pay rise\n",
       "4      20030219      air nz strike to affect australian travellers"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1226258\n",
      "                                       headline_text  index\n",
      "0  aba decides against community broadcasting lic...      0\n",
      "1     act fire witnesses must be aware of defamation      1\n",
      "2     a g calls for infrastructure protection summit      2\n",
      "3           air nz staff in aust strike for pay rise      3\n",
      "4      air nz strike to affect australian travellers      4\n"
     ]
    }
   ],
   "source": [
    "print(len(documents))\n",
    "print(documents[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dataset\n",
    "data = pd.read_json('https://raw.githubusercontent.com/selva86/datasets/master/newsgroups.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rec.autos' 'comp.sys.mac.hardware' 'comp.graphics' 'sci.space'\n",
      " 'talk.politics.guns' 'sci.med' 'comp.sys.ibm.pc.hardware'\n",
      " 'comp.os.ms-windows.misc' 'rec.motorcycles' 'talk.religion.misc'\n",
      " 'misc.forsale' 'alt.atheism' 'sci.electronics' 'comp.windows.x'\n",
      " 'rec.sport.hockey' 'rec.sport.baseball' 'soc.religion.christian'\n",
      " 'talk.politics.mideast' 'talk.politics.misc' 'sci.crypt']\n"
     ]
    }
   ],
   "source": [
    "print(data.target_names.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>target</th>\n",
       "      <th>target_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: lerxst@wam.umd.edu (where's my thing)\\nS...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: guykuo@carson.u.washington.edu (Guy Kuo)...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: twillis@ec.ecn.purdue.edu (Thomas E Will...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From: jgreen@amber (Joe Green)\\nSubject: Re: W...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From: jcm@head-cfa.harvard.edu (Jonathan McDow...</td>\n",
       "      <td>14</td>\n",
       "      <td>sci.space</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  target  \\\n",
       "0  From: lerxst@wam.umd.edu (where's my thing)\\nS...       7   \n",
       "1  From: guykuo@carson.u.washington.edu (Guy Kuo)...       4   \n",
       "2  From: twillis@ec.ecn.purdue.edu (Thomas E Will...       4   \n",
       "3  From: jgreen@amber (Joe Green)\\nSubject: Re: W...       1   \n",
       "4  From: jcm@head-cfa.harvard.edu (Jonathan McDow...      14   \n",
       "\n",
       "            target_names  \n",
       "0              rec.autos  \n",
       "1  comp.sys.mac.hardware  \n",
       "2  comp.sys.mac.hardware  \n",
       "3          comp.graphics  \n",
       "4              sci.space  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly select a subset\n",
    "data = data.sample(n = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to list\n",
    "data_text = data.content.values.tolist()\n",
    "\n",
    "# Remove Emails\n",
    "data_text = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data_text]\n",
    "\n",
    "# Remove new line characters\n",
    "data_text = [re.sub('\\s+', ' ', sent) for sent in data_text]\n",
    "\n",
    "# Remove distracting single quotes\n",
    "data_text = [re.sub(\"\\'\", \"\", sent) for sent in data_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['From: Followups-to: talk.politics.guns Subject: Re: ATF BURNS DIVIDIAN RANCH '\n",
      " '- UPDATE Distribution: usa Lines: 11 Ah yes, I see a few liberal weenies '\n",
      " 'have come out of the woodwork to defend the burning of the children. '\n",
      " 'Probably drooled all over themselves while watching the TV coverage. '\n",
      " 'Probably had a few like that in Nazi Germany, as well. Oh yeah, ATF/FBI now '\n",
      " 'claims, according the the media, that there are a few survivors. The number '\n",
      " 'seems to vary minute by minute. ']\n"
     ]
    }
   ],
   "source": [
    "pprint(data_text[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize words and Clean-up text\n",
    "\n",
    "Let’s tokenize each sentence into a list of words, removing punctuations and unnecessary characters altogether.\n",
    "\n",
    "Gensim’s simple_preprocess() is great for this. Additionally I have set deacc=True to remove the punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize spacy language model\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Customized stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New stop words list \n",
    "customize_stop_words = [\n",
    "    'doi', 'preprint', 'copyright', 'org', 'https', 'et', 'al', 'author', 'figure', 'table', 'e-mail', 'file'\n",
    "    'rights', 'reserved', 'permission', 'use', 'used', 'using', 'biorxiv', 'medrxiv', 'license', 'fig', 'fig.', 'al.', 'Elsevier', 'PMC', 'CZI',\n",
    "    '-PRON-', 'usually'\n",
    "]\n",
    "\n",
    "# Mark them as stop words\n",
    "for w in customize_stop_words:\n",
    "    nlp.vocab[w].is_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check word frequencys\n",
    "def spacy_tokenizer(sentence):\n",
    "    return [word.lemma_ for word in nlp(sentence) if not (word.like_num or word.is_stop or word.is_punct or word.is_space or len(word)==1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(tokenizer = spacy_tokenizer, min_df=2)\n",
    "data_vectorized = vectorizer.fit_transform(data_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='word'>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# most frequent words\n",
    "word_count = pd.DataFrame({'word': vectorizer.get_feature_names(), 'count': np.asarray(data_vectorized.sum(axis=0))[0]})\n",
    "word_count.sort_values('count', ascending=False).set_index('word')[:20].sort_values('count', ascending=True).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert sentences to words\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['from', 'followups', 'to', 'talk', 'politics', 'guns', 'subject', 're', 'atf', 'burns', 'dividian', 'ranch', 'update', 'distribution', 'usa', 'lines', 'ah', 'yes', 'see', 'few', 'liberal', 'weenies', 'have', 'come', 'out', 'of', 'the', 'woodwork', 'to', 'defend', 'the', 'burning', 'of', 'the', 'children', 'probably', 'drooled', 'all', 'over', 'themselves', 'while', 'watching', 'the', 'tv', 'coverage', 'probably', 'had', 'few', 'like', 'that', 'in', 'nazi', 'germany', 'as', 'well', 'oh', 'yeah', 'atf', 'fbi', 'now', 'claims', 'according', 'the', 'the', 'media', 'that', 'there', 'are', 'few', 'survivors', 'the', 'number', 'seems', 'to', 'vary', 'minute', 'by', 'minute']]\n"
     ]
    }
   ],
   "source": [
    "data_words = list(sent_to_words(data_text))\n",
    "print(data_words[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Bigram and Trigram Models\n",
    "\n",
    "Bigrams are two words frequently occurring together in the document. Trigrams are 3 words frequently occurring.\n",
    "\n",
    "Gensim’s Phrases model can build and implement the bigrams, trigrams, quadgrams and more. The two important arguments to Phrases are min_count and threshold. The higher the values of these param, the harder it is for words to be combined to bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['from', 'followups', 'to', 'talk', 'politics', 'guns', 'subject_re', 'atf', 'burns', 'dividian', 'ranch', 'update', 'distribution_usa', 'lines', 'ah', 'yes', 'see', 'few', 'liberal', 'weenies', 'have', 'come', 'out', 'of', 'the', 'woodwork', 'to', 'defend', 'the', 'burning', 'of', 'the', 'children', 'probably', 'drooled', 'all', 'over', 'themselves', 'while', 'watching', 'the', 'tv', 'coverage', 'probably', 'had', 'few', 'like', 'that', 'in', 'nazi', 'germany', 'as', 'well', 'oh', 'yeah', 'atf', 'fbi', 'now', 'claims', 'according', 'the', 'the', 'media', 'that', 'there', 'are', 'few', 'survivors', 'the', 'number', 'seems', 'to', 'vary', 'minute', 'by', 'minute']\n"
     ]
    }
   ],
   "source": [
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Stopwords, Make Bigrams and Lemmatize\n",
    "\n",
    "The bigrams model is ready. Let’s define the functions to remove the stopwords, make bigrams and lemmatization and call them sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['followup', 'talk', 'politic', 'gun', 'atf', 'burn', 'dividian', 'ranch', 'update', 'distribution', 'usa', 'line', 'see', 'liberal', 'weenie', 'come', 'woodwork', 'defend', 'burn', 'child', 'probably', 'drool', 'watch', 'tv', 'coverage', 'probably', 'nazi', 'germany', 'atf', 'fbi', 'claim', 'accord', 'medium', 'survivor', 'number', 'seem', 'vary', 'minute', 'minute']]\n"
     ]
    }
   ],
   "source": [
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the Dictionary and Corpus needed for Topic Modeling\n",
    "\n",
    "The two main inputs to the LDA topic model are the dictionary(id2word) and the corpus.  \n",
    "\n",
    "Gensim creates a unique id for each word in the document. The produced corpus shown above is a mapping of (word_id, word_frequency).\n",
    "\n",
    "For example, (0, 1) above implies, word id 0 occurs once in the first document. Likewise, word id 1 occurs twice and so on.\n",
    "\n",
    "This is used as the input by the LDA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 2), (2, 2), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 2), (19, 1), (20, 1), (21, 1), (22, 2), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1)]]\n"
     ]
    }
   ],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'accord'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2word[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('accord', 1),\n",
       "  ('atf', 2),\n",
       "  ('burn', 2),\n",
       "  ('child', 1),\n",
       "  ('claim', 1),\n",
       "  ('come', 1),\n",
       "  ('coverage', 1),\n",
       "  ('defend', 1),\n",
       "  ('distribution', 1),\n",
       "  ('dividian', 1),\n",
       "  ('drool', 1),\n",
       "  ('fbi', 1),\n",
       "  ('followup', 1),\n",
       "  ('germany', 1),\n",
       "  ('gun', 1),\n",
       "  ('liberal', 1),\n",
       "  ('line', 1),\n",
       "  ('medium', 1),\n",
       "  ('minute', 2),\n",
       "  ('nazi', 1),\n",
       "  ('number', 1),\n",
       "  ('politic', 1),\n",
       "  ('probably', 2),\n",
       "  ('ranch', 1),\n",
       "  ('see', 1),\n",
       "  ('seem', 1),\n",
       "  ('survivor', 1),\n",
       "  ('talk', 1),\n",
       "  ('tv', 1),\n",
       "  ('update', 1),\n",
       "  ('usa', 1),\n",
       "  ('vary', 1),\n",
       "  ('watch', 1),\n",
       "  ('weenie', 1),\n",
       "  ('woodwork', 1)]]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Human readable format of corpus (term-frequency)\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Topic Model\n",
    "\n",
    "We have everything required to train the LDA model. In addition to the corpus and dictionary, you need to provide the number of topics as well.\n",
    "\n",
    "Apart from that, alpha and eta are hyperparameters that affect sparsity of the topics. According to the Gensim docs, both defaults to 1.0/num_topics prior.\n",
    "\n",
    "chunksize is the number of documents to be used in each training chunk. update_every determines how often the model parameters should be updated and passes is the total number of training passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=20, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View the topics in LDA model\n",
    "\n",
    "The above LDA model is built with 20 different topics where each topic is a combination of keywords and each keyword contributes a certain weightage to the topic.\n",
    "\n",
    "You can see the keywords for each topic and the weightage(importance) of each keyword using lda_model.print_topics() as shown next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.011*\"selanne\" + 0.009*\"way\" + 0.009*\"see\" + 0.009*\"do\" + 0.008*\"trace\" + '\n",
      "  '0.008*\"goal\" + 0.006*\"game\" + 0.006*\"line\" + 0.006*\"modem\" + 0.006*\"width\"'),\n",
      " (1,\n",
      "  '0.010*\"line\" + 0.009*\"uranium\" + 0.008*\"write\" + 0.008*\"do\" + '\n",
      "  '0.008*\"organization\" + 0.007*\"article\" + 0.006*\"receiver\" + '\n",
      "  '0.006*\"university\" + 0.005*\"host\" + 0.005*\"nntp_poste\"'),\n",
      " (2,\n",
      "  '0.001*\"file\" + 0.000*\"firearm\" + 0.000*\"gun\" + 0.000*\"get\" + '\n",
      "  '0.000*\"gun_control\" + 0.000*\"article\" + 0.000*\"rkba\" + 0.000*\"bill\" + '\n",
      "  '0.000*\"handgun\" + 0.000*\"mr\"'),\n",
      " (3,\n",
      "  '0.013*\"line\" + 0.013*\"islander\" + 0.011*\"character\" + 0.008*\"truetype\" + '\n",
      "  '0.008*\"font\" + 0.008*\"oem\" + 0.008*\"globe\" + 0.008*\"win\" + 0.005*\"get\" + '\n",
      "  '0.005*\"set\"'),\n",
      " (4,\n",
      "  '0.014*\"administration\" + 0.013*\"key\" + 0.012*\"inquiry\" + 0.012*\"security\" + '\n",
      "  '0.009*\"chip\" + 0.008*\"eff\" + 0.007*\"organization\" + 0.007*\"issue\" + '\n",
      "  '0.007*\"adopt\" + 0.007*\"escrow_agent\"'),\n",
      " (5,\n",
      "  '0.010*\"get\" + 0.009*\"life\" + 0.009*\"organization\" + 0.008*\"vram\" + '\n",
      "  '0.008*\"want\" + 0.008*\"line\" + 0.008*\"write\" + 0.006*\"even\" + 0.005*\"know\" + '\n",
      "  '0.005*\"question\"'),\n",
      " (6,\n",
      "  '0.017*\"satellite\" + 0.016*\"launch\" + 0.010*\"organization\" + 0.010*\"march\" + '\n",
      "  '0.010*\"cosmo\" + 0.008*\"first\" + 0.008*\"electronic\" + 0.008*\"moscow\" + '\n",
      "  '0.008*\"itar\" + 0.008*\"tass\"'),\n",
      " (7,\n",
      "  '0.015*\"keyboard\" + 0.008*\"motherboard\" + 0.008*\"line\" + 0.007*\"system\" + '\n",
      "  '0.007*\"new\" + 0.007*\"do\" + 0.007*\"datadesk\" + 0.006*\"get\" + 0.006*\"gun\" + '\n",
      "  '0.006*\"program\"'),\n",
      " (8,\n",
      "  '0.013*\"upgrade\" + 0.011*\"article\" + 0.009*\"write\" + 0.008*\"information\" + '\n",
      "  '0.007*\"organization\" + 0.007*\"line\" + 0.007*\"product\" + 0.007*\"info\" + '\n",
      "  '0.006*\"do\" + 0.006*\"audio\"'),\n",
      " (9,\n",
      "  '0.024*\"scsi\" + 0.010*\"ide\" + 0.008*\"throat\" + 0.007*\"fbi\" + 0.007*\"drive\" + '\n",
      "  '0.007*\"write\" + 0.007*\"line\" + 0.006*\"say\" + 0.006*\"paul\" + 0.006*\"come\"'),\n",
      " (10,\n",
      "  '0.013*\"tool\" + 0.011*\"rsi\" + 0.010*\"exercise\" + 0.009*\"do\" + 0.009*\"window\" '\n",
      "  '+ 0.009*\"get\" + 0.007*\"include\" + 0.007*\"type\" + 0.007*\"break\" + '\n",
      "  '0.007*\"help\"'),\n",
      " (11,\n",
      "  '0.012*\"get\" + 0.009*\"use\" + 0.009*\"connect\" + 0.009*\"update\" + '\n",
      "  '0.009*\"modem\" + 0.008*\"window\" + 0.008*\"line\" + 0.008*\"black\" + 0.008*\"do\" '\n",
      "  '+ 0.008*\"ford\"'),\n",
      " (12,\n",
      "  '0.016*\"machine\" + 0.010*\"problem\" + 0.010*\"ram\" + 0.010*\"wright\" + '\n",
      "  '0.010*\"gene\" + 0.006*\"card\" + 0.006*\"mac\" + 0.006*\"portable\" + 0.006*\"idea\" '\n",
      "  '+ 0.006*\"instal\"'),\n",
      " (13,\n",
      "  '0.059*\"file\" + 0.015*\"gun\" + 0.011*\"firearm\" + 0.007*\"handgun\" + '\n",
      "  '0.007*\"gun_control\" + 0.006*\"bill\" + 0.006*\"mr\" + 0.006*\"rkba\" + '\n",
      "  '0.005*\"article\" + 0.005*\"crime\"'),\n",
      " (14,\n",
      "  '0.011*\"window\" + 0.011*\"line\" + 0.010*\"key\" + 0.009*\"need\" + '\n",
      "  '0.008*\"organization\" + 0.007*\"phone\" + 0.007*\"nntp_poste\" + 0.007*\"host\" + '\n",
      "  '0.007*\"write\" + 0.006*\"use\"'),\n",
      " (15,\n",
      "  '0.013*\"line\" + 0.013*\"go\" + 0.010*\"organization\" + 0.010*\"schiewer\" + '\n",
      "  '0.008*\"nntp_poste\" + 0.008*\"life\" + 0.008*\"research\" + 0.008*\"inland\" + '\n",
      "  '0.008*\"mar\" + 0.005*\"high\"'),\n",
      " (16,\n",
      "  '0.011*\"auto\" + 0.010*\"spect\" + 0.009*\"kratz\" + 0.008*\"memory\" + 0.007*\"get\" '\n",
      "  '+ 0.007*\"resolution\" + 0.007*\"much\" + 0.006*\"organization\" + 0.006*\"line\" + '\n",
      "  '0.006*\"do\"'),\n",
      " (17,\n",
      "  '0.011*\"mack\" + 0.010*\"say\" + 0.010*\"iran\" + 0.008*\"write\" + 0.008*\"state\" + '\n",
      "  '0.008*\"gulf\" + 0.006*\"article\" + 0.006*\"line\" + 0.006*\"organization\" + '\n",
      "  '0.006*\"mark\"'),\n",
      " (18,\n",
      "  '0.010*\"get\" + 0.010*\"lock\" + 0.009*\"food\" + 0.008*\"bike\" + 0.007*\"do\" + '\n",
      "  '0.007*\"bell\" + 0.007*\"cobra\" + 0.006*\"line\" + 0.006*\"organization\" + '\n",
      "  '0.006*\"write\"'),\n",
      " (19,\n",
      "  '0.020*\"com\" + 0.014*\"get\" + 0.012*\"do\" + 0.010*\"line\" + '\n",
      "  '0.008*\"organization\" + 0.008*\"year\" + 0.008*\"command\" + 0.007*\"m\" + '\n",
      "  '0.007*\"code\" + 0.007*\"surgery\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to interpret this?\n",
    "\n",
    "Topic 0 is a represented as _0.016“car” + 0.014“power” + 0.010“light” + 0.009“drive” + 0.007“mount” + 0.007“controller” + 0.007“cool” + 0.007“engine” + 0.007“back” + ‘0.006“turn”.\n",
    "\n",
    "It means the top 10 keywords that contribute to this topic are: ‘car’, ‘power’, ‘light’.. and so on and the weight of ‘car’ on topic 0 is 0.016.\n",
    "\n",
    "The weights reflect how important a keyword is to that topic.\n",
    "\n",
    "Looking at these keywords, can you guess what this topic could be? You may summarise it either are ‘cars’ or ‘automobiles’.\n",
    "\n",
    "Likewise, can you go through the remaining topic keywords and judge what the topic is?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Model Perplexity and Coherence Score\n",
    "\n",
    "Model perplexity and topic coherence provide a convenient measure to judge how good a given topic model is. In my experience, topic coherence score, in particular, has been more helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -7.919194935792182\n"
     ]
    }
   ],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  0.3934252250309479\n"
     ]
    }
   ],
   "source": [
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the topics-keywords\n",
    "\n",
    "Now that the LDA model is built, the next step is to examine the produced topics and the associated keywords. There is no better tool than pyLDAvis package’s interactive chart and is designed to work well with jupyter notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pyLDAvis' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-3af4665cd12f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Visualize the topics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mvis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mvis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pyLDAvis' is not defined"
     ]
    }
   ],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how to infer pyLDAvis’s output?\n",
    "\n",
    "Each bubble on the left-hand side plot represents a topic. The larger the bubble, the more prevalent is that topic.\n",
    "\n",
    "A good topic model will have fairly big, non-overlapping bubbles scattered throughout the chart instead of being clustered in one quadrant.\n",
    "\n",
    "A model with too many topics, will typically have many overlaps, small sized bubbles clustered in one region of the chart.\n",
    "\n",
    "Alright, if you move the cursor over one of the bubbles, the words and bars on the right-hand side will update. These words are the salient keywords that form the selected topic.\n",
    "\n",
    "We have successfully built a good looking topic model.\n",
    "\n",
    "Given our prior knowledge of the number of natural topics in the document, finding the best model was fairly straightforward.\n",
    "\n",
    "Upnext, we will improve upon this model by using Mallet’s version of LDA algorithm and then we will focus on how to arrive at the optimal number of topics given any large corpus of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
