{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP with BERT Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though word embeddings and recurrent neural networks such as LSTM have proven to be extremely efficient for various natural language processing tasks such as text classification, there are a few problems with LSTM and CNN based text classification systems.  \n",
    "\n",
    "With LSTM, data can only be read in a sequential manner in one direction. Though bidirectional LSTM solves this problem by reading data in both forward and backward directions, the text is still processed sequentially instead of being processed parallelly. This is where transformer models come into play. Transformer models process the whole text document in parallel, relying on attention mechanism.  \n",
    "\n",
    "In the attention mechanism, instead of processing text sequentially, the text is processed in parallel, which allows the attention systems to assign weightage to important parts of the text in a parallel manner.  \n",
    "\n",
    "Several transformer models have been developed until now. However, in this chapter, you will be studying BERT, which stands for (Bidirectional Encoder Representations from Transformers) developed by Google.  \n",
    "\n",
    "**Why use Transformers (BERT)**  \n",
    "Why should we use BERT over traditional word embeddings and LSTM based neural networks?  \n",
    "\n",
    "BERT or Transformer models are able to generate word representations that capture local context. For instance, with word embeddings, the representation of the word “apple” will be the same even if we talk about “apple I phone” or apple as a fruit. With BERT, a different word representation will be generated. In addition, BERT models divide words into stems and leaf segments.  \n",
    "- For instance, the word “Judgmental” is treated as two separate tokens “Judgement” and “al,” which makes word representation more flexible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Text Classification\n",
    "The BERT model is used for generating text representations, which you can then use with LSTM or CNN networks to build text classification or any other type of model. However, BERT also contains Sequence Classification models, which can be used to classify text.  \n",
    "\n",
    "The steps involved in text classification using BERT sequence models are as follows:\n",
    "1. Generate BERT tokens (use the BERT Tokenizer).\n",
    "2. Convert data into the format that can be used by a BERT model and perform word embeddings using BERT.\n",
    "3. Create a sequence classification model for BERT.\n",
    "4. Train the sequence classification model.\n",
    "5. Evaluate the model on the test set.  \n",
    "\n",
    "For this exercise you can download the IMDB sentiment dataset from [kaggle](https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Tokenization and Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer=BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "max_sent_length = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create BERT input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sent= 'Hello, are you judgemental. No I am incremental.'\n",
    "\n",
    "# add start and stop tokens\n",
    "sample_sent_plus_special_tokens = '[CLS]' + sample_sent + '[SEP]'\n",
    "\n",
    "tokenized_sent = bert_tokenizer.tokenize(sample_sent_plus_special_tokens)\n",
    "print('tokenized_sent',tokenized_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- “Judgmental” is treated as two separate tokens “Judgement” and “al”\n",
    "- “incremental” is treated as three separate tokens “inc” and “rem” and \"ental\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the text tokens into the integer format using the **_convert_tokens_to_ids()_** method of the BERT tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = bert_tokenizer.convert_tokens_to_ids(tokenized_sent)\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply padding to the input sentences so that all the sentences have the same length.  \n",
    "Since the maximum sentence length is set to 25 and the input sentence contains 16 tokens, the pad length will be 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_length = max_sent_length - len(input_ids)\n",
    "print(pad_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = input_ids + ([0] * pad_length)\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention masks are a list of 0s and 1s where a 1 is added to the positions that contained original objects, while 0 is added to the padding indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "att_mask = [1] * len(input_ids)\n",
    "att_mask = att_mask + ([0] * pad_length)\n",
    "print(att_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Token type ids are used to separate input sentence from an output sentence in case there are two sentences in the input.  \n",
    "Zeros are added for the index of the first sentence, while a 1 is added for the indexes of the other sentence.  \n",
    "Since we have only a single sentence, we will only create a list of all 0s for token type ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_type_ids = [0] * max_sent_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally the code below create a complete input for BERT models, a dictionary of token ids, token type ids, and attention mask is created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_for_bert = {\n",
    "    \"token_ids\": input_ids, \n",
    "    \"token_type_ids\": token_type_ids, \n",
    "    \"attention_mask\": att_mask\n",
    "}\n",
    "print(input_for_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic BERT input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _**encode_plus**_ function of the BertTokenizer object can  be used to create the BERT input automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_for_bert = bert_tokenizer.encode_plus(\n",
    "    sample_sent, \n",
    "    add_special_tokens = True, \n",
    "    max_length = max_sent_length,\n",
    "    truncation=True,\n",
    "    pad_to_max_length = True,\n",
    "    return_attention_mask = True\n",
    ")\n",
    "\n",
    "print(\"encoded\", input_for_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification  \n",
    "Training is computationally intensive good to use GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Version:\", tf.__version__)\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print(\"Hub version: \", hub.__version__)\n",
    "print(\"GPU is\", \"available\" if tf.test.is_gpu_available() else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount(‘/gdrive’)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_data=pd.read_csv(r\"data/IMDB_Dataset.csv\")\n",
    "imdb_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "def remove_tags(document):\n",
    "    return TAG_RE.sub('', document)\n",
    "\n",
    "\n",
    "def clean_text(doc):\n",
    "    document = remove_tags(doc)\n",
    "    document = re.sub('[^a-zA-Z]', ' ', document)\n",
    "    document = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', document)\n",
    "    document = re.sub(r'\\s+', ' ', document)\n",
    "    \n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_data[\"review_clean\"]=imdb_data[\"review\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_data['sentiment'] = imdb_data['sentiment'].map({'positive': 1, 'negative': 0})\n",
    "imdb_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import (TFBertForSequenceClassification, BertTokenizer)\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = imdb_data[\"review_clean\"].values\n",
    "y = imdb_data['sentiment'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Shape of training data: {0},\\nShape of test data: {1}\".format(X_train.shape, X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create an object of the BertTokenizer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(reviews):\n",
    "    max_length = 0\n",
    "    for review in reviews:\n",
    "        if len(review) > max_length:\n",
    "            max_length = len(review)\n",
    "            \n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_token=0\n",
    "pad_token_segment_id=0\n",
    "\n",
    "max_length= 128\n",
    "max_length = max_length(X_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_bert_input(reviews):\n",
    "    \n",
    "    input_ids,attention_masks,token_type_ids=[],[],[]\n",
    "    for review in tqdm(reviews):\n",
    "        bert_inputs = bert_tokenizer.encode_plus(\n",
    "            review,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,\n",
    "            truncation=True\n",
    "        )\n",
    "    \n",
    "    inputs, token_type = bert_inputs[\"input_ids\"], bert_inputs[\"token_type_ids\"]\n",
    "    mask = [1] * len(inputs)\n",
    "    padding_length = max_length - len(inputs)\n",
    "    \n",
    "    inputs = inputs + ([pad_token] * padding_length)\n",
    "    mask = mask + ([0] * padding_length)\n",
    "    token_type = token_type + ([pad_token_segment_id] * padding_length)\n",
    "    \n",
    "    input_ids.append(inputs)\n",
    "    attention_masks.append(mask)\n",
    "    token_type_ids.append(token_type)\n",
    "\n",
    "    return [np.asarray(input_ids), \n",
    "            np.asarray(attention_masks), \n",
    "            np.asarray(token_type_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_input = text_to_bert_input(X_test)\n",
    "X_train_input = text_to_bert_input(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are using the TensorFlow backend to train BERT models, we need to convert the input data into tensors. The following script does that for both the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tensors(input_ids,attention_masks,token_type_ids, y):\n",
    "    return {\"input_ids\": input_ids, \n",
    "            \"attention_mask\": attention_masks,\n",
    "            \"token_type_ids\": token_type_ids}, y \n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train_input[0], X_train_input[1], X_train_input[2]))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test_input[0],X_test_input[1],X_test_input[2]))\n",
    "#.map(convert_to_tensors).batch(64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you need to define the classification model. With Hugging Face’s transformers library, you can use the TFBertForSequenceClassificaion class to create a text classification model. The process of defining loss, optimizer, and evaluation metrics is similar to any other TensorFlow model. The following script creates a BERT text classification model and displays the model summary in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=TFBertForSequenceClassification.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08)\n",
    "loss= tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "metric=tf.keras.metrics. SparseCategoricalAccuracy('accuracy')\n",
    "model.compile(optimizer=optimizer,loss=loss,metrics=[metric])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_dataset, epochs=1, validation_data=test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
